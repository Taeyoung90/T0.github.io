---

title: "[논문 리뷰] YOLO9000: Better, Faster, Stronger(YOLO v2)"
excerpt: "YOLO v2 논문 리뷰"
last_modified_at: 2021-08-26

categories: 

 - 논문리뷰
 
tags:

 - YOLO
 - AI
 - 논문리뷰
 - VISION

# header:
#   overlay_image: https://images.unsplash.com/photo-1501785888041-af3ef285b470?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1350&q=80
#   overlay_filter: 0.5 # 투명도

toc: true
toc_icon: "bars" # 아이콘 설정
toc_sticky: true #목차를 스크롤과 내릴것인지
toc_label: "목차"
#색상 R:ff4c4c G:34bf49 B:0099e5
---
이번엔 YOLO v2 논문인 [YOLO9000: Better, Faster, Stronger](https://arxiv.org/pdf/1612.08242.pdf)를 읽고 정리해보고자 한다.  
v3보다 먼저 읽어봤지만 github blog를 시작전이라서 이제 포스팅 한다.

 - - -

# Abstract
 - YOLO9000은 9000개의 category를 real-time detection 함
 - multi-scale training method를 사용하여 YOLO v2는 다양한 size에서 detect가 가능하고, 속도와 정확도 trade-off를 쉽게 할 수 있음
 - object detection과 classification을 joint training 하는 방법 제안  
 (COCO detection dataset과 ImageNet classification dataset 같이 train)
 - joint training덕분에 YOLO9000은 label이 없는 detection data의 class 예측함

 <p align="center">
    <img src="https://user-images.githubusercontent.com/73866596/130846298-3b694b45-19a2-49f2-b266-aad2f92aff94.png"><br/>
    <em>출처:YOLO9000: Better, Faster, Stronger</em>
 </p>
<br/>

# 1 Introduction
 - 일반적인 목적의 object detection은 빨라야 하고, 정확해야 하고, 다양한 object를 인식할 수 있어야 함, but 적은 수의 class에 한정되어 있음(classification이나 tagging에 비해서)  
 →<span style="color:#ff4c4c"> 그래서 detection도 classification처럼 scale을 늘리고자 함(더 많은 class를 detection하도록)</span>  
 → but detection labeling은 classification labeling보다 expensive(**detection은 bbox도 지정해야 되서**)<br/>

- hierarchical view of object classification : detection dataset과 classification dataset을 결함
- joint training algorithm : labeled detection image는 object localization, classification image는 vocabulary and robustness 증가에 활용

- 먼저 YOLO v1 → YOLO v2로 향상시키고, 위의 dataset combination method와 joint training으로 YOLO 9000 만듦

# 2 Better
- R-CNN 계역은 box regression과 classification을 따로 하는 two-stage detector  
- YOLO는 Conv. neural network로 한 번에 하는 one-stage detector
- YOLO는 Fast-R-CNN에 비해서 **localization error 더 많고, recall 낮음**  
→ <span style="color:#ff4c4c">v2는 이러한 단점에 집중하되 FPS와 accuracy를 유지하기 위해 모델을 키우기보다는 학습이 쉬운 형태로 간소화함</span>  

## 2.1 Batch Normalization
- Batch normalization(BN)은 overfitting 방지를 위해 사용하는 여러 regularization 기법에 대한 필요성을 없애면서 수렴을 크게 개선
- conv. layer 다음에 BN을 추가해서 mAP 2% 증가
- BN을 넣음으로써 overfitting이 방지되서 dropout 제거

## 2.2 High Resolution Classifier
- v1에서는 224×224로 학습시키고 detection을 위해 448×448로 해상도를 증가시킴
→ 학습 후 detection mode 전환과 동시에 해상도 조정이 이뤄져야 함
- v2에서는 ImageNet의 448×448 해상도에서 10 epoch 동안 분류 network를 finetune함  
→ 이 과정이 filter가 high resolution input에 대해 더 잘 작동하도록 함, mAP 4% 상승

## 2.3 Convolutional With Anchor Boxes
- YOLO v1은 fully-connected(fc) layer에서 bbox coordinates 예측  
대신 Faster R-CNN은 hand-picked priors로 bbox coordinates를 예측하는 region proposal network(RPN) 사용  
Faster R-CNN의 RPN은 conv. layer만 사용하기 때문에 anchor box의 coordinates와 confidence를 예측
- 예측 layer가 convolutional하기 때문에 RPN은 feature map의 모든 위치에서 offset을 예측  
→ offset을 예측하는 것이 문제를 더 간단하게 만들고 network가 학습하는 것을 더 쉽게 함  
<span style="color:#ff4c4c">사전에 정의한 anchor box로부터 bbox offset을 예측하는 것이 기존 v1에서 bbox 위치를 직접 예측하는 것보다 더 쉬움</span>
- **따라서 bbox 예측을 위해 기존의 v1 모델에서 anchor box를 도입하고 FC layer 제거**
    1. 하나의 pooling layer 제거 → conv. layer 출력을 더 고해상도로 
    2. input 이미지 크기를 $448 \times 448$ 에서 $416 \times 416$로 축소  
    → 최종 feature map 크기가 홀수가 되도록 하기 위해  
      object(특히 큰 물체의 경우)는 보통 이미지의 중앙에 위치하는데 중앙의 4개 셀보다 1개의 셀에 위치하는 게 예측하기에 더 좋음
    - YOLO는 32배 downsample 함 → $416 \times 416$ > $13 \times 13$
- v1에서는 grid 별로 object와 class를 예측하지만 v2에서는 anchor box마다 class와 objectness를 예측
v1과 마찬가지로 objectness는 GT와 IOU값을, class prediction은 conditional probaility를 예측

- <span style="color:#ff4c4c">anchor box 사용으로 정확도가 조금 감소</span>  
- v1에서는 각 grid cell당 2개 bbox 예측하여 98개의 bbox 생성, v2에서는 anchor box 사용으로 1000개 이상 예측 가능
- anchor box 사용전 <span style="color:#ff4c4c">69.5 mAP</span>, <span style="color:#0099e5">81% recall</span> >> <span style="color:#ff4c4c">사용후 69.2 mAP</span>, <span style="color:#0099e5">88% recall</span>  
**mAP는 조금 감소했지만, recall이 많이 증가함**

## 2.4 Dimension Clusters
- YOLO에서 anchor box를 사용으로 생기는 문제 2가지(여기서 하나, 다음 절에서 하나)  
    1) Faster R-CNN에서 처럼 단순히 anchor box의 dimension을 사전에 미리 정해서(hadn-picked) 적용함  
    하지만 network가 좀 더 좋게 학습할 수 있게 더 좋은 조건(prior)을 선택한다면 더 좋을 것이라 판단  
    → 따라서 직접 prior를 고르지 않고, k-means clustering을 통해 자동으로 최적의 prior를 찾도록 함  
    k-means clustering에 **Euclidean distance를 적용하면 큰 box가 작은 박스보다 더 큰 error 발생**  
    → 따라서 box의 <span style="color:#ff4c4c">**size에 상관없는 distance metric**</span> 사용  
    
    $$d(box, centroid) = 1 - IOU(box, centroid)$$
    
    k=5일 때 recall과 complexity의 적절한 trade-off 값이라고 판단  
    (k(anchor box수)이 적으면 연산 속도는 증가하지만 anchor box가 제한적이어서 detection성능이 하락, 반대로 k가 크면 성능은 증가하지만 box수가 증가하여 연산 속도 하락)  

<p align="center">
    <img src="https://user-images.githubusercontent.com/73866596/130846391-229b3ab4-01f3-4878-b712-d94f11552ba3.png"><br/>
    <em>출처:YOLO9000: Better, Faster, Stronger</em>
</p>

## 2.5 Direct Location Prediction
anchor box 사용의 두 번째 문제 : 학습 초기 iteration 동안 <span style="color:#ff4c4c">모델이 불안정</span>  
→ box의 중심점(x,y) 예측때문에 
- RPN에서는 $t_x, t_y$ 값을 예측하여 $(x,y)$를 아래와 같이 계산
$$x = (t_x * w_a) - x_a$$  
$$y = (t_y * h_a) - y_a$$

ex) $t_x = 1$이면 anchor box가 width만큼 우측으로 이동, $t_x = 1$이면 좌측으로 이동  

→ 즉 anchor box가 예측하고자 하는 박스와 상관없이 이미지 어느 곳이나 생성가능 함 > random initialization할 경우 적절한 offset을 찾는데 오래 걸림  

- 따라서 x,y를 offset 값을 예측하기보다는, grid cell 위치에 대해 **상대적인 좌표**를 예측  
YOLO는 x,y를 grid cell 내부에 있도록 제한을 두기 때문에 x,y의 GT(ground truth)는 항상 0~1값이 됨  
logistic activation func.를 사용하여 예측값 범위가 0~1이 되도록 함  
- output feature map에서 각 셀마다 5개의 bbox를 예측  
각 box에 대해서 5개의 값($t_x, t_y, t_w, t_h, t_o$) 예측  
$(c_x, c_y)$는 각 그리드 셀의 좌측 상단 끝의 offset  
$(p_w, p_h)$는 bbox prior의 width, height  

$$\begin{aligned}
b_x &= \sigma(t_x) + c_x \\
b_y &= \sigma(t_y) + c_y \\
b_w &= P_w e^{t_w} \\
b_h &= P_h e^{t_h} \\
Pr(objects) * IOU(b, object) &= \sigma(t_o)
\end{aligned}$$

$b_x, b_y, b_w, b_h$는 각 값들을 통해 실제 GT와 IOU를 계산할 최종 bbox의 offset 값들 의미  
location 예측을 parameterization하는 것은 훈련을 더 쉽고 네트워크를 안정화함
- dimension clusters로 bbox 중심 좌표를 직접 예측함으로써 anchor box를 사용한 YOLO보다 5% mAP 상승  

## 2.6 Fine-Grained Features
- $13 \times 13$ feature map에서 detection 수행  
큰 객체에 대해서는 충분하지만, 작은 물체를 localizing 하는 것은 더 세밀한 feature가 좋음  
- Faster R-CNN과 SSD에서는 다양한 크기의 feature map에서 proposal network를 실행함으로써 다양한 해상도 범위를 가능케 함  
- 따라서 앞의 layer($26 \times 26$ 해상도)에서 feature map을 가져오는 <span style="color:#ff4c4c">**passthrough layer**</span>를 추가함  
**passthrough layer**는 고해상도 feature와 저해상도 feature를 concatenate  
(그냥 붙일 수는 없으므로 $26 \times 26 \times 512$를 $13 \times 13 \times 2048$로 바꿔서 붙임)
<p align="center">
    <img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F48d9r%2FbtqTrSVghra%2FvZWjLKoFtduGr6iyf59m51%2Fimg.png" width="400px"><br/>
    <em>출처:https://herbwood.tistory.com/17</em>
</p>

- 이 과정을 통해 성능 1% 증가

## 2.7 Multi-scale Training
- YOLO v2는 conv. layer와 pooling layer로만 구성되어 있기 때문에 여러 해상도의 이미지 input가능
- 다양한 size에 대해서 robust하게 만들기 위해 input size를 바꾸는 대신 몇 번의 iteration마다 network를 바꿈
- 10 batch마다 랜덤하게 image size를 선택
- YOLO가 32배 downsampling하므로 이미지 차원들은 32배수로 선정  
(320, 352, $\dots$, 608)
- 이러한 훈련 방식을 통해서 다양한 해상도 이미지에 대해 잘 deteciton함
- 작은 이미지에 대해서는 더 빠름 > YOLO v2는 속도와 정확도 trade-off가 더 쉬움

<p align="center">
    <img src="https://user-images.githubusercontent.com/73866596/130846820-60bf5417-d9e4-4baf-ba11-d4933ef2fac2.png" title="출처:YOLO9000: Better, Faster, Stronger" width="400"><br/>
    <em>출처:YOLO9000: Better, Faster, Stronger</em>
</p>

## 2.8 Further Experiments
- YOLO v2를 VOC 2012로 훈련, 다른 SOTA 모델에 비해서 빠르면서도 좋은 성능을 보임

<p align="center">
    <img src="https://user-images.githubusercontent.com/73866596/130847082-3bf2fd6f-22b9-4018-b82c-182ab9465e7e.png" title="출처:YOLO9000: Better, Faster, Stronger" width="400"><br/>
    <em>출처:YOLO9000: Better, Faster, Stronger</em>
</p><br/>

*아래 그림은 YOLO v1에서 v2과정으로 추가하거나 수정하면서의 성능 변화를 간략히 나타냄

<p align="center">
    <img src="https://user-images.githubusercontent.com/73866596/130847727-993ce2e4-5db1-46a1-ba81-4a76c8f6aec7.png" title="출처:YOLO9000: Better, Faster, Stronger"><br/>
    <em>출처:YOLO9000: Better, Faster, Stronger</em>
</p>


# 3 Faster
- detection 응용(로보틱스나 자율 주행등)은 **빠른 속도**(low latency)가 중요
- 대부분의 detection framework는 VGG-16을 feature extractor로 사용  
→ powerful, accurate but, **쓸데없이 복잡**
- 그래서 GoogleNet architecture에 기반한 custom network 사용(Darknet-19) > 정확도는 조금 떨어지지만 속도는 빠름
## 3.1 Darknet-19
- Darknet-19구조는 아래와 같음(19개의 conv. layer와 5개 maxpooling layer)
- VGG처럼 $3 \times 3$ filter 사용
<p align="center">
    <img src="https://user-images.githubusercontent.com/73866596/130800235-8d87d3a3-4638-4b19-a761-4c90199afc81.png" title="출처:YOLO9000: Better, Faster, Stronger"><br/>
    <em>출처:YOLO9000: Better, Faster, Stronger</em>
</p><br/>


## 3.2 Training For Classification
- ImageNet 1000 class classification dataset으로 160epoch동안 SGD로 훈련
- learning rate 0.1로 시작, polynomial rate decay with a power of 4, weight decay of 0.0005, momentum of 0.9
- Data augmentation 사용(random crops, rotations, and hue, saturation, and exposure shifts) 
- 초기에는 $224 \times 224$로 훈련하고, $448 \times 448$로 fine tunning
- fine tunning은 10epoch만 learning rate는 $10^{-3}$, 나머지 파라미터는 동일

## 3.3 Training For Detection
- classification network에서 마지막 conv. layer를 $3 \times 3$ conv. layer(1024 filters)로 바꾼뒤  $1 \times 1$ conv. layer를 추가
- $1 \times 1$ conv. layer수는 detection에 필요한 출력 수와 동일
- PASCAL VOC dataset에 대해서 5개의 box, 각 box마다 5개의 coordinates와 20개의 클래스 예측  
→ 125(5×(5 +20)) filter = 마지막 $1 \times 1$ conv. layer의 channel 수
- $3 \times 3 \times 512$ layer로부터 마지막에서 두번째 층에 passthrough layer를 더함 > 모델이 세부적인 feature를 활용할 수 있게 함
- 160epoch, 시작 learning rate $10^{-3}$, 10,60,90 epoch 마다 decay, weight decay 0.0005, momentum 0.9 적용
- Data augmentation(random crops, color shifting, etc.)

# 4 Stronger
- detection dataset과 classification dataset을 섞음
- detection data에 대해서는 network가 YOLO v2의 전체 loss func. 활용
- classification data에 대해서는 network가 loss func. 중 classification part만 활용
- 여기에 몇 가지 challenge가 있음
    * detection dataset은 일반적인 label(개, 보트 등)로 구성
    * classification dataset은 구체적인 label(요크셔테리어, 베들링터테리어, 불독 등)로 구성
    * 대부분의 classification에서는 가능한 카테고리에 대해 probabiliy distribution 계산을 위해 softmax를 사용  
    → softmax는 카테고리가 서로 상호베타적이라는 가정, but '개'와 '불독'은 상호베타적이 아님
    * 이를 위해서 multilabel model 사용

## 4.1 Hierarchical classification
- ImageNet 라벨은 WordNet(개념과 관계를 구조화한 language database)에서 따옴
- WordNet에서 'Norfolk terrier'나 'Yorkshire terrier'은 둘다 사냥개, 개, 가축 등 'terrier'의 하위어
- classification에서는 라벨에 대해서 flat structure지만, detection data와 섞기 위해서 위와 같은 구조가 필요
- WordNet은 트리 형태가 아닌 directed graph 구조('개'는 '개과'이면서도 '가축')(언어는 복잡하기 때문에)
- 이 연구에서는 full graph가 아닌 hierarchical tree 구성  

> WordNet(워드넷)은 영어의 의미 어휘목록이다. WordNet은 영어 단어를 '**synset**'이라는 유의어 집단(동의어 집합)으로 분류하여 간략하고 일반적인 정의를 제공하고, 이러한 어휘목록 사이의 다양한 의미 관계를 기록한다. 워드넷은 자연어 처리(NLP, Natural Language Processing)를 위한 특화된 사전이라고 볼 수 있다.  
출처: https://excelsior-cjh.tistory.com/64

- 트리를 만들기 위해 ImageNet에서 visual noun(시각 명사?)를 조사하고, WordNet graph로 경로를 찾음  
많은 관련어(synset)이 하나의 경로를 갖는 경우가 많으므로 먼저 처리하고 트리를 늘려나감  
('개'아래 '요크셔 테리어', '시츄' 등등 이런 단어들을 개로 묶고 그런 카테고리를 여러개 붙인다는 의미)  
루트에서 특정 경로가 제일 가까운 것부터 선택해 붙여나감
- 워드 트리를 구축하기 위해 1000개의 클래스를 갖는 ImageNet 데이터를 이용해 Darknet-19 모델을 학습  
워드 트리 1k를 만들기 위해서 중간 노드들을 추가해야 되서 라벨 갯수가 1369개로 늘어남
- 이렇게 워드 트리를 만들고 이를 통해 classification을 하기 위해서 모든 노드에 대해 conditional probability를 예측  
- 예를 들어 이미지가 Norfolk terrier인지에 대해 계산하면,  
(classification목적으로 이미지는 객체가 있다고 가정: $Pr(physical object) = 1$)
$$\begin{aligned}
Pr(Norfolk terrier) = Pr(Norfolk terrier|terrier)\\
*Pr(terrier|hunting dog) \\
\dots\\
*Pr(mammal|Pr(animal))\\
*Pr(animal|physical object)
\end{aligned}$$

- conditional probability 계산을 위해 모델은 1369개 값의 벡터를 예측하고, 아래 그림처럼 같은 level끼리 softmax함  
(일반적은 classification은 모든 카테고리에 대해서 softmax로 계산, 하지만 여기서는 앞서 구축한 tree 기준으로 같은 level의 카테고리끼리만 softmax를 함)  
<p align="center">
    <img src="https://user-images.githubusercontent.com/73866596/130830950-ac11af94-efbe-4cdd-bdee-2a448ca093fb.png" title="출처:YOLO9000: Better, Faster, Stronger"><br/>
    <em>출처:YOLO9000: Better, Faster, Stronger</em>
</p>

- 모르는 카테고리에 대해서도 성능은 약간만 떨어짐  
(예를 들어 품종이 불분명한 개 이미지에 대해서 '개'라는 클래스에는 high confidence로 예측하고, 품종만 lower confidence로 예측)
- detection에 대해서도 동일하게 예측(bbox + tree of probabilityes)

## 4.2 Dataset combination with WordTree 
- WordTree를 이용하여 ImageNet과 COCO label을 합침  
WordNet은 extremely diverse하기 때문에 다른 dataset에 대해서도 적용 가능

<p align="center">
<img src="https://user-images.githubusercontent.com/73866596/130833220-0cb70895-6e15-4ae4-8ceb-07bb06359b09.png"><br/>
<em>출처:YOLO9000: Better, Faster, Stronger</em>
</p>

- ImageNet은 COCO dataset보다 훨씬 많으므로 COCO를 4:1로 oversampling하여 맞춤
- 이 데이터셋으로 YOLO9000을 훈련
- YOLO v2 architecture를 사용했지만 ouput size때문에 5개 대신 3개의 prior(bbox)를 예측하게 함

## 4.3 Joint classification and detection
- full ImageNet에서 top 9000 class를 학습시키고자 함
- detection image에 대해서는 평범하게 loss를 backpropagate함  
classification loss에 대해서는 라벨의 일치하는 level의 계층이나 상위계층만 loss를 backpropagate하고 트리의 하위 계층을 예측할 시 error 부과  

- joint training으로 인해서 YOLO9000은 <span style="color:#ff4c4c">COCO dataset으로 detection을 학습하고, ImageNet dataset으로 다양한 객체를 분류하는 학습</span>을 함  
- YOLO9000은 19.7 mAP 성능을 보였고, 본 적이 없는 156개의 클래스를 포함하면 16.0 mAP 나타냄 > 이는 DPM보다 높은 성능
- 또한 9000개의 카테고리를 실시간으로 검출 가능
- 성능 검증할 때 <span style="color:#ff4c4c">동물의 새로운 종은 잘 학습했지만 옷이나 장비에 대해서는 고군분투함</span>
- COCO dataset로부터 동물 객체 검출은 학습했지만, COCO는 옷같은 객체에 대한 bbox label이 없음

<p align="center">
<img src="https://user-images.githubusercontent.com/73866596/130842274-ee125849-2da3-4408-adef-64bdf587665a.png"><br/>
<em>출처:YOLO9000: Better, Faster, Stronger</em>
</p>

# 5 Conclusion
- YOLOv2는 SOTA이면서 다양한 dataset에 대해 다른 detection system보다 빠름
- 속도와 정확도 trade-off가 쉽게 가능하도록 다양한 image size에서 작동
-YOLO9000은 detection과 classification data를 joint training하여 9000개 이상의 객체를 실시간으로 검출가능한 framework임(WordTree 사용)
- hierachical classification을 사용하여 dataset을 합치는 것은 classification과 segmentation에서도 유용할 것으로 예상

# 후기

좀 더 공부가 필요한 부분이 많지만, YOLO v2는 기존의 detection model의 단점인 검출 객체 수를 대폭 증가시키기 위해 joint training을 사용한 점이 흥미로웠음  
detection data로 bbox 예측을 훈련시키고, classification data로 많은 클래스를 classification하도록 훈련시키는 방법이 YOLO9000의 핵심이지 않을까 생각함

- - -
# Reference
- https://gist.github.com/aisolab/be5e2af35d52911b3a2ac519a7c9a0cf
- https://herbwood.tistory.com/17
- https://velog.io/@skhim520/YOLO-v2-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0
 -https://excelsior-cjh.tistory.com/64