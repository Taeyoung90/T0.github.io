---

title: "[Pytorch] 파이토치 첫걸음을 공부하며1(소개~)"
excerpt: "'파이토치 첫걸음'을 공부하며 정리한 내용"
last_modified_at: 2021-08-29

categories: 

 - Pytorch
 
tags:

 - Pytorch
 - AI
 - 필사

# header:
#   overlay_image: https://images.unsplash.com/photo-1501785888041-af3ef285b470?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1350&q=80
#   overlay_filter: 0.5 # 투명도

toc: true
toc_icon: "bars" # 아이콘 설정
toc_sticky: true #목차를 스크롤과 내릴것인지
toc_label: "목차"
#색상 R:ff4c4c G:34bf49 B:0099e5
---

'파이토치 첫걸음' 책을 시작으로 파이토치를 본격적으로 공부해보고자 한다. 텐서플로와 파이토치 모두 각각 장단점이 있다고 하지만, 현재 연구쪽에서 가장 많이 사용되고 있는 프레임워크가 파이토치이고 간편성이 좋다고 하니 주력 프레임워크로 기초를 탄탄히 쌓으려고 한다.
초반에는 기초적인 용어설명이나 원리등을 간단히 정리하고 뒤에서는 직접 코드 구현을 한 내용들을 정리할 생각이다.
- - -

파이토치는 GPU연산을 가능하게 하는 API가 있음(텐서플로도 GPU사용가능)  
- CUDA : 엔비디아가 GPU를 통한 연산을 가능하게 만든 API  
- cuDNN : CUDA를 이용해 딥러닝 연산을 가속해주는 라이브러리
<br/>
  
텐서플로 vs 파이토치
- 텐서플로는 연산 그래프를 먼저 만들고 실제 연산할 때 값을 전달하여 결과를 얻는 **'Define and Run'** 방식  
→ 그래프를 정의하는 부분과 돌리는 부분이 분리 > 코드 길어짐
- 파이토치는 그래프를 만듦과 동시에 값이 할당되는 **'Define by Run'** 방식  
→ 그래프 정의와 동시에 값도 초기화되어 그래프와 연산을 분리해서 생각할 필요 없음  
<br/>

보통 파이토치가 텐서플로보다 2.5배 빠른 연산 속도(상황에 따라 텐서플로가 빠른 환경도 있음)

파이토치 한국 커뮤니티 : PyTorch-Kr 페이스북 그룹

### 설치방법
- 파이썬, CUDA, cuDNN 순서로 설치후, 마지막에 pytorch 설치(colab은 이미 설치되어 있음)
- 더 구체적인 방법은 생략(책이나 구글링 참고)
<br/>

파이토치에서 데이터 기본 단위는 **텐서(tensor)**  
* 텐서 : 다차원 배열(array)
<p align="center">
<img src="https://user-images.githubusercontent.com/73866596/131255267-27357f6a-8a31-4e99-b8e6-571fe01bb26f.png"><br/>
<em>출처: '파이토치 첫걸음'</em>
</p>

`torch.tensor` 함수는 인수로 `data`, `dtype`, `device`, `reauires_grad`등을 받음  
- `data`는 배열이 들어감
- `dtype`은 자료형
- `device`는 이 텐서를 어느 기기에 올릴 것인지 명시
- `requires_grad`는 이 텐서에 대한 기울기 저장 여부(기본 False)
<br/>

아래 코드는 $z= 2x^2 + 3$이라는 식에서 x에 대한 기울기를 구하는 코드
```ruby
#기울기를 계산하는 코드 예시
x = torch.tensor(data=[2.0,3.0], requires_grad=True) 
y = x**2
z= 2*y +3

target = torch.tensor([3.0,4.0])
loss = torch.sum(torch.abs(z-target))
loss.backward()

print(x.grad, y.grad, z.grad)

>> tensor([8., 12.]) None None
```
- x라는 텐서를 생성하며 기울기를 계산하도록 지정
- z와 target의 절대값 차이를 계산하고, `torch.sum()`함수를 통해 $1 \times 2$모양이었던 두 값의 차이를 숫자 하나로 바꿈
- `loss.backward()`함수를 호출하면 연산 그래프를 쭉 따라가면서 leaf node x에 대한 기울기를 계산
*leaf node는 식에 다른 변수를 통해 계산되는 y,z가 아닌 그 자체가 값인 x 같은 노드
<br/>
- x는 leaf node라서 계산되자만, y,z는 None값 리턴

## 선형회귀분석 모델 생성, 기울기 계산

```ruby
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.init as init

num_data = 1000
num_epoch = 500

x = init.uniform_(torch.Tensor(num_data,1), -10,10) # 1000,1 형태 텐서 생성후-10~10으로 uniform하게 초기화
noise = init.normal_(torch.FloatTensor(num_data,1), std=1) #가우시안 노이즈 평균 0(생략), 표준편차 1
y =2*x + 3
y_noise = 2*(x+noise)+3 #실제 데이터들은 noise가 있으므로 실제처럼 랜덤 노이즈 추가

model = nn.Linear(1,1)# 입력 특성수 1, 출력 특성수 1
loss_func = torch.nn.L1Loss()# l1 손실 : 차이절대값 평균

optimizer = optim.SGD(model.parameters(), lr=0.01) 

label = y_noise
for i in range(num_epoch):
  optimizer.zero_grad() #각 반복시 지난번에 계산했던 기울기를 0 으로 초기화 > 새로운 가중치와 편차에 대해서 새로운 기술기를 구할 수 있음
  output = model(x) #모델에 x를 전달하여 결과 output에 저장

  loss = loss_func(output, label)
  loss.backward() #변수 w,b에 대한 기울기가 계산
  optimizer.step() #model.parameters()에서 리턴되는 변수들의 기울기에 학습률 0.01을 곱하여 빼줌으로써 업데이트

  if i % 1 ==0:
    print(loss.data)

param_list = list(model.parameters())
print(param_list[0].item(), param_list[1].item())
```
- 실행하면 2,3에 가까운 값이 나타남

***

## 인공신경망으로 회귀분석

- 위의 코드와 차이점이라면 model을 신경망으로 구성한 것 > 모델 생성이 굉장히 간략함을 느낄 수 있음

```ruby
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.init as init

num_data = 1000
num_epoch =  10000

noise = init.normal_(torch.FloatTensor(num_data,1), std=1)
x = init.uniform_(torch.Tensor(num_data,1), -15,15)
y = (x**2) +3
y_noise = y+ noise

model = nn.Sequential(
    nn.Linear(1,6),
    nn.ReLU(),
    nn.Linear(6,10),
    nn.ReLU(),
    nn.Linear(10,6),
    nn.ReLU(),
    nn.Linear(6,1)
)

loss_func = nn.L1Loss()
optimizer = optim.SGD(model.parameters(), lr=0.0002)

loss_array = []
for i in range(num_epoch):
  optimizer.zero_grad()
  output= model(x)
  loss = loss_func(output, y_noise)
  loss.backward()
  optimizer.step()

  loss_array.append(loss)

```

## 합성곱 신경망
- feature map 크기 계산
    * I : 이미지 크기
    * K : 필터 크기
    * S : 스트라이드
    * P : 패딩
    * floor : 버림함수  

$$O = floor\left( \frac{I-K+2P}{S} +1\right)$$  
<br/>

- ReLU 함수는 0이하의 값에 대해서는 0으로 출력하고 그 이상의 값(0이상)은 그대로 출력함  
→ 자극(입력값)이 0이상이면 그대로 전달하기 때문에 전파되는 값들이 크고 역전파되는 값 역시 y=x를 미분하면 1이 나오기 때문에 기울기 값 그대로 전파되어 학습 속도 빠름  
→ 단점 : 다잉 뉴런(dying neuron)현상 발생 ex) 가중치와 편차가 -5, -3이고 입력값이 정규화되어 -1~1이면 출력값이 항상 0보다 작기 때문에 뉴런은 영원히 업데이트 되지 않음  
- ReLU의 약점을 보완하기 위해 나온게 leaky ReLU, randomized leaky ReLU  
0이하의 값에 대해서도 약하게 값을 출력

<p align="center">
<img src="https://user-images.githubusercontent.com/73866596/131258637-2540469d-0d1f-4994-aa05-a54dbf7a5c02.png"><br/>
<em>출처: '파이토치 첫걸음'</em>
</p>

- - -
오늘은 파이토치에 대한 간단한 소개와 기본적인 개념 맛보기 정도였고, 책 분량이 길지 않아서 빠르게 훑으면서 친숙해져보고 다음 책으로 넘어가야 겠다.