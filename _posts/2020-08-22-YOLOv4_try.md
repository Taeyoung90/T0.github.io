---

title: "[모델적용] YOLO v4 object detection 해보기 with 마스크 데이터(colab)"

last_modified_at: 2021-08-22

categories: 

 - 모델적용
 
tags:

 - YOLO
 - AI
 - Object detection
 - VISION

# header:
#   overlay_image: https://images.unsplash.com/photo-1501785888041-af3ef285b470?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1350&q=80
#   overlay_filter: 0.5 # 투명도

toc: true
toc_icon: "bars" # 아이콘 설정
toc_sticky: true #목차를 스크롤과 내릴것인지
toc_label: "목차"

---
YOLO v4에 대해서 간단히 익혀보자(colab 사용)
Colab의 GPU를 사용  
YOLO v4 pretrained 모델을 사용하여 마스크의 착용여부를 detection하는 모델을 만들어본다.    
참고: [Train a custom YOLO v4 Object detector(Using Google Colab)](https://medium.com/analytics-vidhya/train-a-custom-yolov4-object-detector-using-google-colab-61a659d4868)

-----

## 1. 구글 드라이브에 yolov4 폴더 생성, 그리고 그 안에 training 폴더 생성  
trained weights를 저장할 위치(obj.data파일)를 위해서 구글드라이브에 ~yolov4/training/ 생성  

![drive1](https://user-images.githubusercontent.com/73866596/130315589-c883dc6e-e81b-46b7-8bf4-b3d94cd08f89.png)

## 2. 구글 드라이브 마운트 및 yolov4 폴더 위치 설정
```
%cd ..
from google.colab import drive
drive.mount('/content/gdrive')
```
`/content/gdrive/My\ Drive/`이 `/mydrive` 되도록 변경
```
!ln -s /content/gdrive/My\ Drive/ /mydrive
```
`/mydrive/yolov4` 디렉토리 이동
```
%cd /mydrive/yolov4
```
## 3. YOLO v4 git clone(현재 디렉토리에)
```
!git clone https://github.com/AlexeyAB/darknet
```
아래와 같이 드라이브에 darknet 폴더가 복사됨
![drive2](https://user-images.githubusercontent.com/73866596/130315604-ebef5edb-669c-401d-8fdf-d5623bcae7a5.png)

## 4. Detector training 파일 설정
필요한 파일(뒤에서 하나씩 준비함)
- Labeled custom dataset
- custom cfg file(darknet/cfg 폴더에서 다운로드하여 수정)
- obj.data, obj.names files
- process.py file(to create train.txt and test.txt files for training)
<br/>
원하는 데이터셋(이미지)를 image labeling tool(labelImg, OpenLabeling 등)을 이용하여 라벨링하여 jpg + txt(class와 bounding box coordinates) 데이터를 만듦(수동작업)  
혹은 라벨링된 데이터셋 사이트를 이용()
<br/>

![garbageingarbageout](https://user-images.githubusercontent.com/73866596/130317205-7efa3f81-bea4-4d95-9afb-7d2d76b97108.png)

<span style="color:#ff4c4c">**쓰레기를 넣으면 쓰레기가 나온다.** 데이터를 다루는데 있어서 항상 기억해야 할 말로 이 예제의 저자도 언급했다. 좋은 모델도 중요하지만 그만큼 모델에 넣을 데이터도 중요하니 항상 잊지말자.!!!</span><br/><br/>

- custom **config file**을 생성하고 드라이브 yolov4 폴더에 업로드  
(AlexeyAB's Github 에서 yolov4-custom.cfg 파일을 다운로드 하여 수정하기)<br/><br/>
- [convolutional]의 filters는 (class+5)×3으로 변경(본 예제에서는 class가 2개이므로 21)
- batch는 한 iteration당 처리하는 image
- subdivision이 8이면 batch를 8등분하여 mini-batch로 나눠서 GPU에서 처리함, 따라서 하드웨어 성능이 낮다면, subdivision을 32나 64로 올려서 진행할 것

<p align="center"><img src="https://miro.medium.com/max/500/1*kUED-L1EtThFrse2HzzyMA.png" height="500px"></p>



<p align="center"><img src="https://miro.medium.com/max/1000/1*bXY7eMznH9rAeVTsXYgO5A.png" height="400px"></p>

- obj.zip은 데이터셋 압축파일(여기서는 예제의 저자가 사용한 파일을 다운로드하여 업로드, [obj.zip](https://www.kaggle.com/techzizou/labeled-mask-dataset-yolo-darknet))
- obj.data와 obj.names 파일을 생성하고 드라이브에 업로드<br/>
```
#obj.data 파일
classes = 2
train = data/train.txt #train.txt 파일 위치
valid = data/test.txt #test.txt 파일 위치
names = data/obj.names #obj.names 위치
backup = /mydrive/yolov4/training #weights가 저장될 곳
```
```
#obj.names 파일(클래스 이름이 적힌 파일)
with_mask
without_mask
```
<br/>

<process.py> 파일은 데이터(이미지)를 90% 훈련데이터와 10% 테스트데이터로 나누고, train.txt와 test.txt 파일을 생성, 각각의 파일은 해당 이미지의 path를 의미함  
 <span style="color:#ff4c4c">**process.py 파일은 jpg만 지원 png,jpeg등 다른 형식은 안됨, 다른 형식인 경우 process.py 안에서 .jpg를 다른 형식으로 바꿀 것**</span> 

```
# process.py 내용, 다른 이미지 포맷인 경우 jpg를 바꿀것
import glob, os

# Current directory
current_dir = os.path.dirname(os.path.abspath(__file__))

print(current_dir)

current_dir = 'data/obj'

# Percentage of images to be used for the test set
percentage_test = 10;

# Create and/or truncate train.txt and test.txt
file_train = open('data/train.txt', 'w')
file_test = open('data/test.txt', 'w')

# Populate train.txt and test.txt
counter = 1
index_test = round(100 / percentage_test)
for pathAndFilename in glob.iglob(os.path.join(current_dir, "*.jpg")):
    title, ext = os.path.splitext(os.path.basename(pathAndFilename))

    if counter == index_test:
        counter = 1
        file_test.write("data/obj" + "/" + title + '.jpg' + "\n")
    else:
        file_train.write("data/obj" + "/" + title + '.jpg' + "\n")
        counter = counter + 1
```
<br/>
모두 준비하면 아래와 같은 상태가 됨

<p align="center"><img src="https://user-images.githubusercontent.com/73866596/130330066-63faef58-d1bc-48ad-8dfb-fc016d4fcb65.png" height="400px"></p>

## 5. OPENCV와 GPU 활성화를 위해 makefile 수정
CUDNN, CUDNN_HALF, LIBSO to 1로 설정
```
%cd darknet/
!sed -i 's/OPENCV=0/OPENCV=1/' Makefile
!sed -i 's/GPU=0/GPU=1/' Makefile
!sed -i 's/CUDNN=0/CUDNN=1/' Makefile
!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile
!sed -i 's/LIBSO=0/LIBSO=1/' Makefile
```

## 6. darknet build
위에서 현재 디렉토리 위치를 `mydrive/yolov4/darknet/`로 지정하고 변경하지 않은 상태에서 진행, 위치를 바꿨다면 다시 위치를 변경하고 진행할 것
```
!make
```

## 7. 'yolov4' 폴더를 'darknet' 디렉토리로 복사


```
%cd data/
!find -maxdepth 1 -type f -exec rm -rf {} \;
%cd ..

%rm -rf cfg/
%mkdir cfg
```

- obj.zip을 /data/에 압축을 품
```
!unzip /mydrive/yolov4/obj.zip -d data/
```
- cfg파일을 darknet/cfg/에 복사
```
!cp /mydrive/yolov4/yolov4-custom.cfg cfg
```
- obj.names와 obj.data 파일을 /darknet/data/폴더로 복사
```
!cp /mydrive/yolov4/obj.names data
!cp /mydrive/yolov4/obj.data data
```
- process.py파일도 darknet폴더에 복사
```
!cp /mydrive/yolov4/process.py
```

## 8. process.py로 train, test 나누기
-process.py 실행(현재 디렉토리가 /content/gdrive/My Drive/yolov4/darknet인 상태에서 실행)
```
!python process.py
```
- 제대로 실행됐는지 확인
```
!ls data/ #obj.data와 obj.names, train.txt, test.txt 생성 확인
```

## 9. pretrained weight 다운로드
-transfer learning을 사용함. 모델을 scratch부터 훈련하는 대신 pre-trained YOLOv4 가중치(137개의 컨볼루셔널 레이어로 훈련된)를 이용함
```
!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137
```
## 10. Training
- 성능을 위해 average loss가 0.05보다 낮을 때 훈련을 멈추는게 좋음(최소한 0.3 아래일때)
또는 loss가 더이상 변하지 않을 때까지 훈련
```
!./darknet detector train data/obj.data cfg/yolov4-custom.cfg yolov4.conv.137 -dont_show -map
```
-아래 코드는 중간에 중지하고 이어서 훈련을 진행할 때 (step 2,5,6 하고나서 진행)
```
!./darknet detector train data/obj.data cfg/yolov4-custom.cfg /mydrive/yolov4/training/yolov4-custom_last.weights -dont_show -map
```

<span style="color:#ff4c4c">\* 훈련시 cuda error: out of memory 발생 >> subdivision을 16에서 32로 증가시켜서 해결함</span>  

## 11. Check Performance
- chart.png 파일을 통해 trained weights의 성능을 확인할수 있다. 하지만 chart.png파일은 중간에 간섭하지 않으면 결과만 나타냄.
```
def imShow(path):

  import cv2
  import matplotlib.pyplot as plt
  %matplotlib inline
  image = cv2.imread(path)
  height, width = image.shape[:2]
  resized_image = cv2.resize(image, (3*width, 3*height), interpolation = cv2.INTER_CUBIC)
  fig = plt.gcf()
  fig.set_size_inches(18,10)
  plt.axis('off')
  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))
  plt.show()
```
```
imShow('chart.png')
```
아래와 같이 iteration에 따른 성능을 확인할 수 있음
<p align="center"><img src="https://user-images.githubusercontent.com/73866596/130330960-c16cd22c-a877-4a26-98e5-2c9e95675e5b.png" height="600px"></p>

-mAP 확인(xxxx에 마지막 iteration을 적으면됨 1000단위)
```
!./darknet detector map data/obj.data cfg/yolov4-custom.cfg /mydrive/yolov4/training/yolov4-custom_xxxx.weights -points 0
```

## 12. Test object detector

- test를 하기 위해서 cfg 파일을 수정  
    * change line batch to batch =1
    * change line batch to subdivisions to subdivisions=1

```
%cd cfg
!sed -i 's/batch=64/batch=1/' yolov4-custom.cfg
!sed -i 's/subdivisions=32/subdivisions=1/' yolov4-custom.cfg
%cd ..
```

- image에서 detect 해보기
```
!./darknet detector test data/obj.data cfg/yolov4-custom.cfg /mydrive/yolov4/training/yolov4-custom_best.weights /content/gdrive/MyDrive/yolov4/mask_test_images/test_image1.jpg -thresh 0.3
imShow('predictions.jpg')#이미지 확인용
```
<p align="center"><img src="https://user-images.githubusercontent.com/73866596/130331452-90ea1d0f-615b-4910-b4a9-aa62a05281ba.png" height="400px"></p>
<p align="center"><img src="https://user-images.githubusercontent.com/73866596/130331471-7cfc6657-d1d5-4176-bdd1-969be0ee6f93.png" height="400px"></p>

- ## 웹캠으로 테스트 해보기
- 웹캠으로 사진을 찍고 바로 detection 함
```
from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);
      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});
      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();
      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);
      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);
      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)
  data = eval_js('takePhoto({})'.format(quality))
  binary = b64decode(data.split(',')[1])
  with open(filename, 'wb') as f:
    f.write(binary)
  return filename

from IPython.display import Image
try:
  filename = take_photo()
  print('Saved to {}'.format(filename))
  
  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))

!./darknet detector test data/obj.data cfg/yolov4-custom.cfg /mydrive/yolov4/training/yolov4-custom_best.weights photo.jpg -thresh 0.3
imShow('predictions.jpg')  
```

- ## 영상 파일 detection
```
!./darknet detector demo data/obj.data cfg/yolov4-custom.cfg /mydrive/yolov4/training/yolov4-custom_best.weights -dont_show /mydrive/mask_test_videos/test_video3.mp4 -thresh 0.5 -i 0 -out_filename /mydrive/mask_test_videos/results1.avi
```
- ## 웹캠으로 실시간 detection

```
# Code from theAIGuysCode Github (https://github.com/theAIGuysCode/YOLOv4-Cloud-Tutorial/blob/master/yolov4_webcam.ipynb)
# Adjusted for my custom YOLOv4 trained weights, config and obj.data files

# import dependencies
from IPython.display import display, Javascript, Image
from google.colab.output import eval_js
from google.colab.patches import cv2_imshow
from base64 import b64decode, b64encode
import cv2
import numpy as np
import PIL
import io
import html
import time
import matplotlib.pyplot as plt
%matplotlib inline


# import darknet functions to perform object detections
from darknet import *
# load in our YOLOv4 architecture network
network, class_names, class_colors = load_network("cfg/yolov4-custom.cfg", "data/obj.data", "/mydrive/yolov4/training/yolov4-custom_best.weights")
width = network_width(network)
height = network_height(network)

# darknet helper function to run detection on image
def darknet_helper(img, width, height):
  darknet_image = make_image(width, height, 3)
  img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  img_resized = cv2.resize(img_rgb, (width, height),
                              interpolation=cv2.INTER_LINEAR)

  # get image ratios to convert bounding boxes to proper size
  img_height, img_width, _ = img.shape
  width_ratio = img_width/width
  height_ratio = img_height/height

  # run model on darknet style image to get detections
  copy_image_from_bytes(darknet_image, img_resized.tobytes())
  detections = detect_image(network, class_names, darknet_image)
  free_image(darknet_image)
  return detections, width_ratio, height_ratio

# function to convert the JavaScript object into an OpenCV image
def js_to_image(js_reply):
  """
  Params:
          js_reply: JavaScript object containing image from webcam
  Returns:
          img: OpenCV BGR image
  """
  # decode base64 image
  image_bytes = b64decode(js_reply.split(',')[1])
  # convert bytes to numpy array
  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)
  # decode numpy array into OpenCV BGR image
  img = cv2.imdecode(jpg_as_np, flags=1)

  return img

# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream
def bbox_to_bytes(bbox_array):
  """
  Params:
          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.
  Returns:
        bytes: Base64 image byte string
  """
  # convert array into PIL image
  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')
  iobuf = io.BytesIO()
  # format bbox into png for return
  bbox_PIL.save(iobuf, format='png')
  # format return string
  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))

  return bbox_bytes  

# JavaScript to properly create our live video stream using our webcam as input
def video_stream():
  js = Javascript('''
    var video;
    var div = null;
    var stream;
    var captureCanvas;
    var imgElement;
    var labelElement;
    
    var pendingResolve = null;
    var shutdown = false;
    
    function removeDom() {
       stream.getVideoTracks()[0].stop();
       video.remove();
       div.remove();
       video = null;
       div = null;
       stream = null;
       imgElement = null;
       captureCanvas = null;
       labelElement = null;
    }
    
    function onAnimationFrame() {
      if (!shutdown) {
        window.requestAnimationFrame(onAnimationFrame);
      }
      if (pendingResolve) {
        var result = "";
        if (!shutdown) {
          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);
          result = captureCanvas.toDataURL('image/jpeg', 0.8)
        }
        var lp = pendingResolve;
        pendingResolve = null;
        lp(result);
      }
    }
    
    async function createDom() {
      if (div !== null) {
        return stream;
      }

      div = document.createElement('div');
      div.style.border = '2px solid black';
      div.style.padding = '3px';
      div.style.width = '100%';
      div.style.maxWidth = '600px';
      document.body.appendChild(div);
      
      const modelOut = document.createElement('div');
      modelOut.innerHTML = "<span>Status:</span>";
      labelElement = document.createElement('span');
      labelElement.innerText = 'No data';
      labelElement.style.fontWeight = 'bold';
      modelOut.appendChild(labelElement);
      div.appendChild(modelOut);
           
      video = document.createElement('video');
      video.style.display = 'block';
      video.width = div.clientWidth - 6;
      video.setAttribute('playsinline', '');
      video.onclick = () => { shutdown = true; };
      stream = await navigator.mediaDevices.getUserMedia(
          {video: { facingMode: "environment"}});
      div.appendChild(video);

      imgElement = document.createElement('img');
      imgElement.style.position = 'absolute';
      imgElement.style.zIndex = 1;
      imgElement.onclick = () => { shutdown = true; };
      div.appendChild(imgElement);
      
      const instruction = document.createElement('div');
      instruction.innerHTML = 
          '<span style="color: red; font-weight: bold;">' +
          'When finished, click here or on the video to stop this demo</span>';
      div.appendChild(instruction);
      instruction.onclick = () => { shutdown = true; };
      
      video.srcObject = stream;
      await video.play();

      captureCanvas = document.createElement('canvas');
      captureCanvas.width = 640; //video.videoWidth;
      captureCanvas.height = 480; //video.videoHeight;
      window.requestAnimationFrame(onAnimationFrame);
      
      return stream;
    }
    async function stream_frame(label, imgData) {
      if (shutdown) {
        removeDom();
        shutdown = false;
        return '';
      }

      var preCreate = Date.now();
      stream = await createDom();
      
      var preShow = Date.now();
      if (label != "") {
        labelElement.innerHTML = label;
      }
            
      if (imgData != "") {
        var videoRect = video.getClientRects()[0];
        imgElement.style.top = videoRect.top + "px";
        imgElement.style.left = videoRect.left + "px";
        imgElement.style.width = videoRect.width + "px";
        imgElement.style.height = videoRect.height + "px";
        imgElement.src = imgData;
      }
      
      var preCapture = Date.now();
      var result = await new Promise(function(resolve, reject) {
        pendingResolve = resolve;
      });
      shutdown = false;
      
      return {'create': preShow - preCreate, 
              'show': preCapture - preShow, 
              'capture': Date.now() - preCapture,
              'img': result};
    }
    ''')

  display(js)
  
def video_frame(label, bbox):
  data = eval_js('stream_frame("{}", "{}")'.format(label, bbox))
  return data

# start streaming video from webcam
video_stream()
# label for video
label_html = 'Capturing...'
# initialze bounding box to empty
bbox = ''
count = 0 
while True:
    js_reply = video_frame(label_html, bbox)
    if not js_reply:
        break

    # convert JS response to OpenCV Image
    frame = js_to_image(js_reply["img"])

    # create transparent overlay for bounding box
    bbox_array = np.zeros([480,640,4], dtype=np.uint8)

    # call our darknet helper on video frame
    detections, width_ratio, height_ratio = darknet_helper(frame, width, height)

    # loop through detections and draw them on transparent overlay image
    for label, confidence, bbox in detections:
      left, top, right, bottom = bbox2points(bbox)
      left, top, right, bottom = int(left * width_ratio), int(top * height_ratio), int(right * width_ratio), int(bottom * height_ratio)
      bbox_array = cv2.rectangle(bbox_array, (left, top), (right, bottom), class_colors[label], 2)
      bbox_array = cv2.putText(bbox_array, "{} [{:.2f}]".format(label, float(confidence)),
                        (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,
                        class_colors[label], 2)

    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255
    # convert overlay of bbox into bytes
    bbox_bytes = bbox_to_bytes(bbox_array)
    # update bbox so next frame gets new overlay
    bbox = bbox_bytes   
```

## 후기
- 현재 yolo v5가 나오기도 했고 object detection에서는 우수한 성능으로 많이 사용되고 있는 yolo를 한 번 체험해보고자 간단한 예제 가이드를 따라 진행함
- 클래스가 2개뿐이었지만 적은 훈련으로도 좋은 성능임을 알 수 있었음
- 이후 yolo 논문을 모두 읽어보고 좀 더 상세히 파악 필요