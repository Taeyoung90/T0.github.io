---

title: "[논문 리뷰] You Only Look Once: Unified, Real-Time Object Detection(YOLO v1)"

last_modified_at: 2021-08-20

categories: 

 - 논문리뷰
 
tags:

 - YOLO
 - AI
 - 논문리뷰
 - VISION

# header:
#   overlay_image: https://images.unsplash.com/photo-1501785888041-af3ef285b470?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1350&q=80
#   overlay_filter: 0.5 # 투명도

toc: true
toc_icon: "bars" # 아이콘 설정
toc_sticky: true #목차를 스크롤과 내릴것인지
toc_label: "목차"

---

Computer vision 분야에서 빠질 수 없는 YOLO에 대해서 공부해보고자 한다.  
모두 이해하진 못해도 기록으로 남기고 추후에도 보면서 참고하거나 수정할 수 있다.

-----
## Abstract
 - 과거에는 classifier를 변형하여 detection을 했다.
 - 본 연구에서는 object detection을 regression 문제로 생각한다. > bouding box(bbox)의 위치와 class 확률
 - 하나의 network가 한 번에 full image에서 bbox를 예측하고 class 확률을 예측한다.
 <span style="color:#ff4c4c">(전체 pipeline이 하나의 network로 구성되어 performance가 증가한다. > 한 번에 두가지를 예측하기 때문에) </span>
 - SOTA에 비하면 <span style="color:#ff4c4c">localization error가 더 많지만, FP(False Positive) 예측은 더 낮음 </span>
 (잘못된 양성 예측을 잘 안함)
 - 다른 도메인에 적용시, DPM이나 R-CNN과 같은 다른 method를 능가

## Introduction  
 사람의 시각은 fast, accurate 하면서 complex task가 가능(생각하면서 운전하기 같은)
 - Deformable part model(DPM)은 `sliding window` 방식으로 detection
 - R-CNN은 `region proposal` 방식으로 bbox 생성  
 → bbox에 classifier 적용하여 분류  
 → bbox rescale, 중복 제거(후처리과정)
 → object에 따라 box의 score 측정    
 
 R-CNN과정은 복잡하여 느림, 각 절차를 독립적으로 훈련해야 하기때문에 최적화도 번거러움<br/>
 - Image pixel로부터 bbox coordinates(위치), class 확률을 구하는 과정을 regression problem으로 재정의하는 게 포인트<br/><br/>
 장점
    1. extremely fast
    2. 이미지 전체를 보고 prediction하기 때문에 주변 정보(contextual information)까지 학습<br/>
    (Fast-R-CNN은 주변정보 처리 못하기 때문에 **background error** 높음)<br/>
    *background error : 물체가 없는 배경에 반점이나 노이즈가 있을 때 물체로 잘못 인식하는 것<br/>
    3. 물체의 일반적인 부분 학습 > 새로운 이미지에 대해 robust<br/>  

    단점
    1. SOTA 객체 검출 모델에 비해 <span style="color:#ff4c4c">정확도 낮음 </span><br/>
    (속도와 정확도는 `trade-off` 관계)<br/><br/>

## Unified Detection
- YOLO는 detection의 개별 요소(bbox, class)를 single neural network로 통합한 모델
- 이미지 전체 특징으로 bbox 예측 > high accuracy + end to end training + real time detection
- 과정<br/>
    1. 이미지를 S×S 그리드로 나눔<br/>
    어떤 객체의 중심이 특정 그리드 셀에 위치하면, 그 그리드셍이 해당 객체를 검출해야함
    2. 각각 그리드셀은 **B개의 bbox**와 **confidence score** 예측<br/>
    bbox가 객체를 포함하는 것에 대한 신뢰정도  + bbox가 얼마나 정확한지 의미<br/>
    bbox 예측값 > x,y,w,h, confidence(5개, 그리드 기준 상대적 값 0~1)<br/>
    ex) bbox 중심이 그리드 셀의 중심에 위치하면 (x, y) = (0.5,0.5)
  
    $$Pr(Object)*IOU_{pred}^{truth}$$     
  
    3. 각각의 그리드셀은 conditional class probabilities(C) 예측<br/>
    그리드셀 안에 객체가 있다는 조건하에 어떤 class인지에 대한 조건부 확률<br/>
    그리드 셀에 bbox가 몇 개인지와 상관없이 하나의 그리드 셀에는 하나의 class에 대한 확률 값을 구함
    $$C = Pr(class_i|Object)$$
    
    4. test 단계에서는 conditional class probability(C)와 개별 bbox의 confidence score를 종합 > bbox에 대한 class - specific confidence score<br/>
    $$ Pr(class_i|Object) * Pr(Object) * IOU_{pred}^{truth} $$
    $$ = Pr(class_i) * IOU_{pred}^{truth} $$
     - <span style="color:#ff4c4c">bbox에 특정 class 객체가 있을 확률, 예측된 bbox가 클래스 객체에 얼마나 잘 들어 맞는지 의미</span><br/>

## Network Design
  
- 앞단은 Convolutional layer(이미지 특징 추출)이고 이어서 fully-connected layer(클래스 확률 + bbox coordinates 예측)로 구성
- GoogLeNet에서 가져옴
- 24개 conv. layer + 2개의 fc layer
- GoogLeNet의 Inception 구조 대신 (1×1 reductino layer + 3×3 conv. layer 사용)<br/><br/>
![netwrok_architecture](https://user-images.githubusercontent.com/73866596/130206003-c83eb756-94f4-4992-bec8-e47cf4bcdfcb.png)<br/>

## Training
- 1000개 class ImageNet dataset으로 YOLO의 conv. layer 를 pretrain>br/>
(24개 중에서 처음 20개만 사용후, fc layer 연결)
- training과 inference를 위해 Darknet 프레임워크 사용<br/>
_*Darknet framework : 신경망을 학습하거나 실행 할 수 있는 프레임워크_<br/>
- Pretrain conv. layer 20개 + 4개 conv. layer + 2개 fc layer 추가(뒤에 6개 layer는 가중치 랜덤 초기화시킴)<br/>
-object detection을 위해서는 Image resolution이 높아야 함<br/>
(224×224 > 448×448)<br/>
-(w,h,x,y)를 0~1로 정규화
- 마지막 layer는 linear activation function사용, 나머지는 leaky ReLU 적용<br/>
*_leaky ReLU는 ReLU와 달리 음수값에 대해 작은 값을 출력_ <br/>
- loss는 Sum squared error(SSE) 사용 > 최적화가 쉽기 때문  
하지만 최종목표인 mAP를 높이는 것과 완벽히 일치하지는 않음<br/><br/>
loss = localization loss(bbox를 얼마나 잘 예측했는지) + classification loss(클래스를 얼마나 잘 예측했는지)<br/>
두개의 loss 가중치를 동일하게 두고 학습시키는 것은 좋지 않지만, SSE 최적화 방식에서는 동일하게 취급함<br/>
- 문제1 : 이미지 내 대부분의 그리드 셀에는 객체가 없음(배경이 전경보다 더 크기 때문)  
\> confidence score = 0<br/>
즉 대부분의 그리드 셀이 confidence =0으로 학습하고 이는 모델의 불균형을 초래함
- 개선 : 객체가 존재하는 bbox coordinates에 대한 confidence  loss 가중치를 증가시키고, 객체가 존재하지 않는 bbox의 confidence loss에 대한 가중치 감소시킴<br/>
**localization loss 가중치를 증가시키고, 객체가 존재하는 그리드 셀의 confidence loss의 가중치를 증가시킨다는 의미**
$$\lambda_{coord} = 5 , \lambda_{noobj} = 0.5$$
- 문제2 : SSE는 큰 bbox와 작은 bbox를 동일한 가중치로 loss 계산<br/>
**but 작은 bbox가 작은 위치 변화에 더 민감**<br/>
<span style="color:#ff4c4c">(큰 객체를 둘러싸는 bbox는 조금 움직여도 여전히 객체를 잘 감싸지만 작은 객체를 둘러싸는 bbox는 조금 움직이면 작은 객체를 벗어날 수 있음)</span><br/>
이를 개선하기 위해 bbox의 w,h에 square root를 취함<br/>
<span style="color:#ff4c4c">(w,h에 square root를 취하면 w,h가 커짐에 따라 그 증가율이 감소하여 loss에 대한 가중치를 감소시키는 효과가 있음)</span><br/>
-- root그래프 그림 추가
- YOLO는 하나의 그리드 셀당 여러 개의 bbox 예측<br/>
→ 훈련 단계에서 하나의 bbox predictor가 하나의 object에 대한 책임이 있어야 함  
→ 즉, object 하나 당 하나의 bbox 매칭  
→ 여러개의 bbox 중 하나 선택해야 함  
→ 실제 ground-truth bbox(실제 라벨링된 bbox)와 **IOU가 가장 큰 것 선택**

## Loss function
loss function을 나눠서 살펴보자<br/><br/> 
![loss_function](https://user-images.githubusercontent.com/73866596/130206085-3cb423a2-9a16-49fd-b68b-d655df68199a.png)<br/>
1. Object가 존재하는 그리드 셀 i의 bbox predictor j에 대해, x와 y의 loss 값  
2. Object가 존재하는 그리드 셀 i의 bbox predictor j에 대해, w와 h의 loss 값  
큰 bbox에 대해 small deviation을 반영하기 위해, square root 적용  
<span style="color:#ff4c4c">(같은 error라도 큰 bbox의 경우 상대적으로 IOU에 영향을 적게 줌)</span>
3. object가 존재하는 그리드 셀 i의 bbox predictor j에 대해, confidence score loss 계산($C_i = 1$)
4. object가 존재하지 않는 그리드 셀 i의 bbox predictor j에 대해, confidence score loss 계산($C_i = 0$)
5. object가 존재하는 그리드 셀 i에 대해, conditional class probability의 loss 계산  
$\lambda_coord$ : (x,y,w,h)에 대한 loss와 다른 loss간의 균형을 위한 balancing parameter  
$\lambda_noobj$ : object가 있는 bbox와 없는 bbox 간의 균형을 위한 balancing parameter  
<span style="color:#ff4c4c">(일반적으로 객체가 있는 그리드 셀보다 없는 셀이 더 많기 때문)</span> 
- 처음에는 learning rate를 0.001dptj 0.01로 천천히 상승시킴  
<span style="color:#ff4c4c">(처음부터 높으면 기울기 폭발(gradient explosion)이 발생할 수 있음)</span>
- overfitting 방지를 위해 dropout(0.5)와 data augmentation(random scaling, random transition) 적용

## Inference
- train처럼 추론에서도 객체를 detection하는데 하나의 신경망만 사용하면 됨 > 매우 빠름
- 파스칼 VOC dataset에 대해 YOLO는 이미지 당 98개(7×7 그리드 당 2개)의 bbox 예측+ bbox 클래스 확률
- YOLO grid design의 단점 존재  
'하나의 객체를 여러 그리드 셀이 동시에 검출하는 경우'  
객체의 크기가 크거나 객체가 그리드 셀 경계에 인접한 경우, 그 객체에 대한 bbox가 여러 개 생길 수 있음 > 다중 검출(multiple detection)
→ 비 최대 억제(non-maximal suppression)으로 개선 가능

## Limitation
- YOLO는 하나의 그리드 셀마다 2개의 bbox 예측, but **하나의 객체만 detection 가능**  
→ <span style="color:#ff4c4c">그리드 셀에 두 개이상의 객체가 있으면 잘 검출 못함(공간적 제약, spatial constraints)</span>  
- YOLO 모델은 data로부터 bbox 예측을 학습하기 때문에 훈련 단계에서 학습하지 못했던 새로운 종횡비(aspect ratio)에 대해 고전함
- YOLO 모델은 큰 bbox와 작은 bbox의 loss에 대해 동일한 가중치를 둠  
(작은 bbox 위치가 조금만 변해도 IOU변화가 더 심해서 서능에 더 큰 영향을 미침)  
→ 부정확한 localization 문제

## Comparison to other detection systems
- DPM 
    * Sliding window 방식 사용
    * DPM은 서로 분리된 pipeline(feature extraction, region classification, bbox prediction)으로 구성  
    → 반면 YOLO는 하나의 CNN으로 대체한 모델(한 번에 처리) > 더 빠름
- R-CNN
    * region proposal 방식 사용  
    selective search 방식을 통해 여러 bbox 생성, conv. 신경망으로 feature 추출, SUM으로 bbox에 대한 score 측정, 선형 모델(linear model)로 bbox 조정, 비 최대 억제(non-max suppression)으로 중복 검출 제거  
    → 복잡한 파이프 라인을 단계별로 독립적으로 튜닝해야 함 > 정확도는 높지만 **느리기 때문에 real time detection은 한계 있음**

## Experiment
- Fast-R-CNN은 R-CNN보다 **높은 정확도**+빠름이지만, 그래도 0.5fps로 실시간은 역부족
- Faster-R-CNN은 bbox proposal을 위해 selective search 대신 신경망 사용  
(기존의 R-CNN계열보다 빠르지만 YOLO보다는 느림)

## VOC 2007 Error Analysis
- YOLO는 localization error(class는 일치, 0.1 < IOU < 0.5)가 큼
- Fast-R-CNN은 background error(어떤 obj도 IOU <0.1)가 큼  

## Combining Fast-R-CNN and YOLO
두 모델 앙상블은 YOLO 단독보다는 느리지만 YOLO가 빨라서 Fast-R-CNN을 단독으로 하는 것과 속도가 비슷하기 때문에 Fast-R-CNN을 사용할 거라면 YOLO와 앙상블하는게 더 좋음(정확도 측면에서)

## Conclusion
- YOLO는 구조가 단순하면서도 빠르고 정확
- 새로운 이미지에 대해서도 성능이 좋음(robust)
- 예술작품 data로 테스트할 때 정확도가 많이 안 떨어짐(다른 이미지 데이터에 비해서)